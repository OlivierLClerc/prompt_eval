{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f90e9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe051047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 0.  Load the data\n",
    "# ------------------------------------------------------------------\n",
    "df = pd.read_csv(\"data/full_results_llm_gpt4o_new_exo.csv\", sep=\";\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# Data obtained from the streamlit app are in the xlsx format\n",
    "# df = pd.read_excel(\"data/complete_analysis_results.xlsx\", engine=\"openpyxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a782a1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Code        model     prompt  count\n",
      "0      1  GPT-4o mini    correct     10\n",
      "1      1  GPT-4o mini  incorrect     10\n",
      "2      2  GPT-4o mini    correct     10\n",
      "3      2  GPT-4o mini  incorrect     10\n",
      "4      3  GPT-4o mini    correct     10\n",
      "5      3  GPT-4o mini  incorrect     10\n",
      "6      4  GPT-4o mini    correct     10\n",
      "7      4  GPT-4o mini  incorrect     10\n",
      "8      5  GPT-4o mini    correct     10\n",
      "9      5  GPT-4o mini  incorrect     10\n",
      "10     6  GPT-4o mini    correct     10\n",
      "11     6  GPT-4o mini  incorrect     10\n",
      "12     7  GPT-4o mini    correct     10\n",
      "13     7  GPT-4o mini  incorrect     10\n",
      "14     8  GPT-4o mini    correct     10\n",
      "15     8  GPT-4o mini  incorrect     10\n",
      "16     9  GPT-4o mini    correct     10\n",
      "17     9  GPT-4o mini  incorrect     10\n",
      "18    10  GPT-4o mini    correct     10\n",
      "19    10  GPT-4o mini  incorrect     10\n",
      "20    11  GPT-4o mini    correct     10\n",
      "21    11  GPT-4o mini  incorrect     10\n",
      "22    12  GPT-4o mini    correct     10\n",
      "23    12  GPT-4o mini  incorrect     10\n",
      "24    13  GPT-4o mini    correct     10\n",
      "25    13  GPT-4o mini  incorrect     10\n",
      "26    14  GPT-4o mini    correct     10\n",
      "27    14  GPT-4o mini  incorrect     10\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# ) COUNT THE NUMBER OF RECORDS BY EXERCISE, MODEL, AND PROMPT\n",
    "# -------------------------------------------------\n",
    "# Group by 'Code', 'model', and 'prompt', and then count the occurrences\n",
    "counts_df = df.groupby(['Code', 'model', 'prompt']).size().reset_index(name='count')\n",
    "\n",
    "# Print out the counts\n",
    "print(counts_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d846ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Grade per model × exercise =====\n",
      "prompt        model  Code  correct  incorrect  Overall\n",
      "0       GPT-4o mini     1     19.0        0.0     19.5\n",
      "1       GPT-4o mini     2     20.0        0.0     20.0\n",
      "2       GPT-4o mini     3     20.0        0.0     20.0\n",
      "3       GPT-4o mini     4     20.0        0.0     20.0\n",
      "4       GPT-4o mini     5     20.0       10.0     15.0\n",
      "5       GPT-4o mini     6     20.0        0.0     20.0\n",
      "6       GPT-4o mini     7     19.0        3.0     18.0\n",
      "7       GPT-4o mini     8     20.0        0.0     20.0\n",
      "8       GPT-4o mini     9     20.0        8.0     16.0\n",
      "9       GPT-4o mini    10     20.0        0.0     20.0\n",
      "10      GPT-4o mini    11     20.0        0.0     20.0\n",
      "11      GPT-4o mini    12     20.0        0.0     20.0\n",
      "12      GPT-4o mini    13     20.0        0.0     20.0\n",
      "13      GPT-4o mini    14     20.0        0.0     20.0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1.  Map the 0/1/2 labels to points\n",
    "#     (→ change the dict if we want to treat “1” differently)\n",
    "# ------------------------------------------------------------------\n",
    "label_to_points = {0: 0, 1: 1, 2: 2}\n",
    "df[\"points\"] = df[\"ModelPrediction\"].map(label_to_points)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Raw grade on 20 for every (model × exercise × prompt‑type)\n",
    "# ------------------------------------------------------------------\n",
    "agg = (\n",
    "    df\n",
    "      .groupby([\"model\", \"Code\", \"prompt\"], as_index=False)\n",
    "      .agg(raw_grade=(\"points\", \"sum\"))         # 10 gens × max 2 = 0…20\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  “Good‑ised” grade — so that *higher = better* for both prompts\n",
    "#     Correct prompt  ➜  keep the raw score\n",
    "#     Incorrect prompt➜  invert: goodised = 20 – raw\n",
    "# ------------------------------------------------------------------\n",
    "agg[\"goodised_grade\"] = np.where(\n",
    "    agg[\"prompt\"] == \"incorrect\",\n",
    "    20 - agg[\"raw_grade\"],\n",
    "    agg[\"raw_grade\"]\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Pivot to get both prompt types side‑by‑side, then compute\n",
    "#     a Balanced‑Prompt Grade (BPG) that fuses them\n",
    "# ------------------------------------------------------------------\n",
    "pivot = (\n",
    "    agg\n",
    "      .pivot_table(index=[\"model\", \"Code\"],\n",
    "                   columns=\"prompt\",\n",
    "                   values=\"raw_grade\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "pivot[\"Overall\"] = 0.5 * (pivot[\"correct\"] + (20 - pivot[\"incorrect\"]))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Model‑level summary across all exercises\n",
    "# ------------------------------------------------------------------\n",
    "model_summary = (\n",
    "    pivot\n",
    "      .groupby(\"model\", as_index=False)\n",
    "      .agg(mean_Overall=(\"Overall\", \"mean\"),\n",
    "           good_prompt_mean=(\"correct\", \"mean\"),\n",
    "           bad_prompt_mean=(\"incorrect\", \"mean\"))\n",
    "      .sort_values(\"mean_Overall\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"===== Grade per model × exercise =====\")\n",
    "print(pivot.head(50))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22165f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model‑level summary =====\n",
      "         model  mean_Overall  good_prompt_mean  bad_prompt_mean\n",
      "0  GPT-4o mini     19.178571         19.857143              1.5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== Model‑level summary =====\")\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e775c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_counts(\n",
    "#     data: pd.DataFrame,\n",
    "#     model: str | None = None,\n",
    "#     exo: int | None = None,\n",
    "#     prompt: str | None = None,\n",
    "# ) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Return the number of 0-, 1- and 2-labels in ModelPrediction\n",
    "#     after optionally filtering by model, exercise (Code) and prompt type.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data   : your full_results_llm DataFrame\n",
    "#     model  : exact value from the 'model' column     (e.g. \"GPT-4o mini\")\n",
    "#     exo    : exercise number from the 'Code' column  (e.g. 7)\n",
    "#     prompt : \"correct\" or \"incorrect\"\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.Series with the counts, indexed by [0, 1, 2]\n",
    "#     \"\"\"\n",
    "#     subset = data\n",
    "#     if model is not None:\n",
    "#         subset = subset[subset[\"model\"] == model]\n",
    "#     if exo is not None:\n",
    "#         subset = subset[subset[\"Code\"] == exo]\n",
    "#     if prompt is not None:\n",
    "#         subset = subset[subset[\"prompt\"] == prompt]\n",
    "\n",
    "#     # ensure all three labels appear even if their count is zero\n",
    "#     return (\n",
    "#         subset[\"ModelPrediction\"]\n",
    "#         .value_counts()\n",
    "#         .reindex([0, 1, 2], fill_value=0)\n",
    "#         .astype(int)\n",
    "#         .rename(\"count\")\n",
    "#     )\n",
    "\n",
    "# print(label_counts(df, model=\"GPT-4o mini\", exo=1, prompt=\"incorrect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0da2ac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    exo  mean_overall  GPT-4o mini\n",
      "0     2          20.0         20.0\n",
      "1     3          20.0         20.0\n",
      "2     6          20.0         20.0\n",
      "3     4          20.0         20.0\n",
      "4    10          20.0         20.0\n",
      "5     8          20.0         20.0\n",
      "6    13          20.0         20.0\n",
      "7    14          20.0         20.0\n",
      "8    12          20.0         20.0\n",
      "9    11          20.0         20.0\n",
      "10    1          19.5         19.5\n",
      "11    7          18.0         18.0\n",
      "12    9          16.0         16.0\n",
      "13    5          15.0         15.0\n"
     ]
    }
   ],
   "source": [
    "def rank_exercises_overall(\n",
    "    pivot_df: pd.DataFrame,\n",
    "    overall_col: str = \"Overall\",\n",
    "    ascending: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ranks exercises (exo) by the average 'Overall' score across all models,\n",
    "    and includes the per-model values for comparison.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pivot_df     : DataFrame from step 4 (must include 'model', 'Code', and 'overall_col')\n",
    "    overall_col  : Name of the column representing the overall score (default = 'BPG')\n",
    "    ascending    : Whether to sort from worst to best (default = False)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with:\n",
    "    - 'exo' (exercise ID)\n",
    "    - 'mean_overall'\n",
    "    - one column per model (showing that model’s score on each exercise)\n",
    "    \"\"\"\n",
    "    # Pivot: rows = Code (exo), columns = model\n",
    "    wide = pivot_df.pivot(index=\"Code\", columns=\"model\", values=overall_col)\n",
    "\n",
    "    # Drop the name \"model\" from columns if present\n",
    "    wide.columns.name = None\n",
    "\n",
    "    # Compute the mean and reformat\n",
    "    wide[\"mean_overall\"] = wide.mean(axis=1)\n",
    "    wide = wide.reset_index().rename(columns={\"Code\": \"exo\"})\n",
    "\n",
    "    # Reorder columns: exo, mean, then each model\n",
    "    cols = [\"exo\", \"mean_overall\"] + [col for col in wide.columns if col not in [\"exo\", \"mean_overall\"]]\n",
    "    result = wide[cols].sort_values(\"mean_overall\", ascending=ascending).reset_index(drop=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "exercise_ranking = rank_exercises_overall(pivot)\n",
    "print(exercise_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "978fcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_exercises_by_bpg(\n",
    "    pivot_df: pd.DataFrame,\n",
    "    model: str,\n",
    "    ascending: bool = False    # False = best→worst, True = worst→best\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return the pivot rows for one model, sorted by the Balanced-Prompt Grade.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pivot_df  : the dataframe produced in step 4 (one row = model × exercise)\n",
    "    model     : exact model name as it appears in the 'model' column\n",
    "    ascending : set to True if you want worst → best instead\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with columns ['Code', 'correct', 'incorrect', 'BPG'], sorted.\n",
    "    \"\"\"\n",
    "    df = pivot_df.copy()\n",
    "    df.columns.name = None  # clear the annoying \"prompt\" column label if it exists\n",
    "    return (\n",
    "        df\n",
    "        .loc[df[\"model\"] == model, [\"Code\", \"correct\", \"incorrect\", \"Overall\"]]\n",
    "        .rename(columns={\"Code\": \"exo\"})\n",
    "        .sort_values(\"Overall\", ascending=ascending)\n",
    "        .reset_index(drop=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e3c5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== GPT-4o mini ranking =====\n",
      "    exo  correct  incorrect  Overall\n",
      "0     2     20.0        0.0     20.0\n",
      "1     3     20.0        0.0     20.0\n",
      "2     6     20.0        0.0     20.0\n",
      "3     4     20.0        0.0     20.0\n",
      "4    10     20.0        0.0     20.0\n",
      "5     8     20.0        0.0     20.0\n",
      "6    13     20.0        0.0     20.0\n",
      "7    14     20.0        0.0     20.0\n",
      "8    12     20.0        0.0     20.0\n",
      "9    11     20.0        0.0     20.0\n",
      "10    1     19.0        0.0     19.5\n",
      "11    7     19.0        3.0     18.0\n",
      "12    9     20.0        8.0     16.0\n",
      "13    5     20.0       10.0     15.0\n"
     ]
    }
   ],
   "source": [
    "gpt4o_ranking = rank_exercises_by_bpg(pivot, model=\"GPT-4o mini\")\n",
    "print(\"\\n===== GPT-4o mini ranking =====\")\n",
    "print(gpt4o_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030fd25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Mistral Small 3 ranking =====\n",
      "Empty DataFrame\n",
      "Columns: [exo, correct, incorrect, Overall]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "mistrals3_ranking = rank_exercises_by_bpg(pivot, model=\"Mistral Small 3\")\n",
    "print(\"\\n===== Mistral Small 3 ranking =====\")\n",
    "print(mistrals3_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a811a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Llama 3.3 70B ranking =====\n",
      "Empty DataFrame\n",
      "Columns: [exo, correct, incorrect, Overall]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "llama3_ranking = rank_exercises_by_bpg(pivot, model=\"Llama 3.3 70B\")\n",
    "print(\"\\n===== Llama 3.3 70B ranking =====\")\n",
    "print(llama3_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e1ee4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model     prompt         mean         std\n",
      "0      GPT-4o mini    correct   868.341667  370.700098\n",
      "1      GPT-4o mini  incorrect  1382.825000  415.676803\n",
      "2    Llama 3.3 70B    correct  1227.375000  313.801791\n",
      "3    Llama 3.3 70B  incorrect  1386.008333  329.533373\n",
      "4  Mistral Small 3    correct  1408.558333  717.093817\n",
      "5  Mistral Small 3  incorrect  1885.200000  407.792079\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 6) COMPUTE THE LENGTH OF 'réponse_llm' AND GROUP STATS\n",
    "# -------------------------------------------------\n",
    "# Add a new column 'llm_length' that is the character count of 'réponse_llm'\n",
    "df['llm_length'] = df['réponse_llm'].apply(len)\n",
    "\n",
    "# Group the data by model, prompt type, and exercise code, then compute mean and std deviation \n",
    "grouped_stats = df.groupby(['model', 'prompt'])['llm_length'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Print out the grouped statistics\n",
    "print(grouped_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0cdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Code            model     prompt  mean_similarity  std_similarity\n",
      "0      1      GPT-4o mini    correct         0.489160        0.223464\n",
      "1      1      GPT-4o mini  incorrect         0.705769        0.192325\n",
      "2      1    Llama 3.3 70B    correct         0.337619        0.246238\n",
      "3      1    Llama 3.3 70B  incorrect         0.560682        0.330597\n",
      "4      1  Mistral Small 3    correct         0.913635        0.112648\n",
      "5      1  Mistral Small 3  incorrect         0.921882        0.157887\n",
      "6      2      GPT-4o mini    correct         0.954432        0.063967\n",
      "7      2      GPT-4o mini  incorrect         0.639566        0.224334\n",
      "8      2    Llama 3.3 70B    correct         0.279527        0.180718\n",
      "9      2    Llama 3.3 70B  incorrect         0.416526        0.280307\n",
      "10     2  Mistral Small 3    correct         0.725395        0.351781\n",
      "11     2  Mistral Small 3  incorrect         0.743388        0.320780\n",
      "12     3      GPT-4o mini    correct         0.341372        0.229097\n",
      "13     3      GPT-4o mini  incorrect         0.385764        0.221202\n",
      "14     3    Llama 3.3 70B    correct         0.459680        0.246253\n",
      "15     3    Llama 3.3 70B  incorrect         0.432296        0.183996\n",
      "16     3  Mistral Small 3    correct         0.983779        0.032441\n",
      "17     3  Mistral Small 3  incorrect         0.930540        0.139663\n",
      "18     4      GPT-4o mini    correct         0.685411        0.233352\n",
      "19     4      GPT-4o mini  incorrect         0.356833        0.136083\n",
      "20     4    Llama 3.3 70B    correct         0.257105        0.125528\n",
      "21     4    Llama 3.3 70B  incorrect         0.202029        0.149477\n",
      "22     4  Mistral Small 3    correct         1.000000        0.000000\n",
      "23     4  Mistral Small 3  incorrect         0.705729        0.321787\n",
      "24     5      GPT-4o mini    correct         0.364786        0.287845\n",
      "25     5      GPT-4o mini  incorrect         0.556254        0.211126\n",
      "26     5    Llama 3.3 70B    correct         0.211889        0.211612\n",
      "27     5    Llama 3.3 70B  incorrect         0.680224        0.190105\n",
      "28     5  Mistral Small 3    correct         0.992264        0.010415\n",
      "29     5  Mistral Small 3  incorrect         0.910000        0.118112\n",
      "30     6      GPT-4o mini    correct         0.477795        0.158756\n",
      "31     6      GPT-4o mini  incorrect         0.466058        0.179598\n",
      "32     6    Llama 3.3 70B    correct         0.565243        0.256833\n",
      "33     6    Llama 3.3 70B  incorrect         0.456943        0.210002\n",
      "34     6  Mistral Small 3    correct         0.577336        0.408892\n",
      "35     6  Mistral Small 3  incorrect         0.566942        0.358408\n",
      "36     7      GPT-4o mini    correct         0.316051        0.100841\n",
      "37     7      GPT-4o mini  incorrect         0.344420        0.145203\n",
      "38     7    Llama 3.3 70B    correct         0.378406        0.147490\n",
      "39     7    Llama 3.3 70B  incorrect         0.377638        0.177145\n",
      "40     7  Mistral Small 3    correct         1.000000        0.000000\n",
      "41     7  Mistral Small 3  incorrect         0.975766        0.046068\n",
      "42     8      GPT-4o mini    correct         0.508686        0.243702\n",
      "43     8      GPT-4o mini  incorrect         0.282926        0.162448\n",
      "44     8    Llama 3.3 70B    correct         0.604855        0.155193\n",
      "45     8    Llama 3.3 70B  incorrect         0.454303        0.262163\n",
      "46     8  Mistral Small 3    correct         0.971930        0.030105\n",
      "47     8  Mistral Small 3  incorrect         0.776708        0.215940\n",
      "48     9      GPT-4o mini    correct         0.666441        0.185039\n",
      "49     9      GPT-4o mini  incorrect         0.371270        0.200077\n",
      "50     9    Llama 3.3 70B    correct         0.868644        0.251971\n",
      "51     9    Llama 3.3 70B  incorrect         0.983929        0.015543\n",
      "52     9  Mistral Small 3    correct         0.995967        0.003506\n",
      "53     9  Mistral Small 3  incorrect         0.988346        0.021736\n",
      "54    10      GPT-4o mini    correct         0.708926        0.256662\n",
      "55    10      GPT-4o mini  incorrect         0.461370        0.216321\n",
      "56    10    Llama 3.3 70B    correct         0.847422        0.128900\n",
      "57    10    Llama 3.3 70B  incorrect         0.296421        0.127617\n",
      "58    10  Mistral Small 3    correct         0.992227        0.007335\n",
      "59    10  Mistral Small 3  incorrect         0.793803        0.234991\n",
      "60    11      GPT-4o mini    correct         0.652184        0.213318\n",
      "61    11      GPT-4o mini  incorrect         0.658486        0.224291\n",
      "62    11    Llama 3.3 70B    correct         0.693390        0.299276\n",
      "63    11    Llama 3.3 70B  incorrect         0.317663        0.225739\n",
      "64    11  Mistral Small 3    correct         0.937116        0.067226\n",
      "65    11  Mistral Small 3  incorrect         0.637625        0.350315\n",
      "66    12      GPT-4o mini    correct         0.467237        0.273339\n",
      "67    12      GPT-4o mini  incorrect         0.675806        0.220102\n",
      "68    12    Llama 3.3 70B    correct         0.346206        0.135770\n",
      "69    12    Llama 3.3 70B  incorrect         0.443156        0.215926\n",
      "70    12  Mistral Small 3    correct         0.938676        0.122704\n",
      "71    12  Mistral Small 3  incorrect         0.703337        0.184287\n"
     ]
    }
   ],
   "source": [
    "# # Function to compute similarity ratio between two strings.\n",
    "# def similarity_score(a, b):\n",
    "#     if pd.isna(a) or pd.isna(b):\n",
    "#         return 0.0\n",
    "#     return SequenceMatcher(None, a.strip().lower(), b.strip().lower()).ratio()\n",
    "\n",
    "# # Function to compute mean and standard deviation for a list of responses.\n",
    "# def compute_similarity_stats(responses):\n",
    "#     # Return NaNs if fewer than two responses exist.\n",
    "#     if len(responses) < 2:\n",
    "#         return {'mean_similarity': np.nan, 'std_similarity': np.nan}\n",
    "    \n",
    "#     # Calculate similarity scores for every unique pair.\n",
    "#     scores = [similarity_score(a, b) for a, b in itertools.combinations(responses, 2)]\n",
    "    \n",
    "#     return {'mean_similarity': np.mean(scores), 'std_similarity': np.std(scores)}\n",
    "\n",
    "# # Group the DataFrame by Code, model, and prompt.\n",
    "# similarity_stats = df.groupby(['Code', 'model', 'prompt'])['réponse_llm'].agg(\n",
    "#     mean_similarity=lambda x: compute_similarity_stats(list(x))['mean_similarity'],\n",
    "#     std_similarity=lambda x: compute_similarity_stats(list(x))['std_similarity']\n",
    "# ).reset_index()\n",
    "\n",
    "# # Print the similarity statistics DataFrame.\n",
    "# print(similarity_stats.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model     prompt  consistency\n",
      "0      GPT-4o mini    correct     0.552707\n",
      "1      GPT-4o mini  incorrect     0.492043\n",
      "2    Llama 3.3 70B    correct     0.487499\n",
      "3    Llama 3.3 70B  incorrect     0.468484\n",
      "4  Mistral Small 3    correct     0.919027\n",
      "5  Mistral Small 3  incorrect     0.804506\n"
     ]
    }
   ],
   "source": [
    "# # ------------------------------------------------------------\n",
    "# # Compute the overall mean (mean of means) by model.\n",
    "# # ------------------------------------------------------------\n",
    "# model_mean_similarity = similarity_stats.groupby(['model', 'prompt'])['mean_similarity'].mean().reset_index(name='consistency')\n",
    "\n",
    "# # Print the DataFrame with the model-level mean similarity.\n",
    "# print(model_mean_similarity.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROMPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
